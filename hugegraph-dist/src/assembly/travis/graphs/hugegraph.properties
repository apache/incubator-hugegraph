# gremlin entrance to create graph
# none auth config: com.baidu.hugegraph.HugeFactory
# auth config: com.baidu.hugegraph.auth.HugeFactoryAuthProxy
gremlin.graph=com.baidu.hugegraph.HugeFactory

# The data store type, allow: rocksdb and hstore
backend=rocksdb

# The serializer for backend store: binary
serializer=binary

# The database name like Cassandra Keyspace
store=hugegraph

# Choose a text analyzer for searching the vertex/edge properties, available type are [word, ansj, hanlp, smartcn, jieba, jcseg, mmseg4j, ikanalyzer]
# Specify the mode for the text analyzer, the available mode of analyzer are
# word: [MaximumMatching, ReverseMaximumMatching, MinimumMatching, ReverseMinimumMatching, BidirectionalMaximumMatching, BidirectionalMinimumMatching, BidirectionalMaximumMinimumMatching, FullSegmentation, MinimalWordCount, MaxNgramScore, PureEnglish]
# ansj: [BaseAnalysis, IndexAnalysis, ToAnalysis, NlpAnalysis]
# hanlp: [standard, nlp, index, nShort, shortest, speed]
# smartcn: []
# jieba: [SEARCH, INDEX]
# jcseg: [Simple, Complex]
# mmseg4j: [Simple, Complex, MaxWord]
# ikanalyzer: [smart, max_word]
search.text_analyzer=jieba
search.text_analyzer_mode=INDEX

# The max rate(items/s) to add/update/delete vertices/edges
#rate_limit.write=0

# The max rate(times/s) to execute query of vertices/edges
#rate_limit.read=0

# Timeout in seconds for waiting for the task to complete, such as when truncating or clearing the backend
# task.wait_timeout=10

# The job input size limit in bytes
#task.input_size_limit=

# The job result size limit in bytes
#task.result_size_limit=

# The batch size used to delete expired data
#task.ttl_delete_batch=1

# Whether to delete schema or expired data synchronously
#task.sync_deletion=false

# The interval in seconds for detecting connections, if the idle time of a connection exceeds this value
# detect it and reconnect if needed before using, value 0 means detecting every time
#store.connection_detect_interval=600

# The default vertex label
#vertex.default_label=vertex

# Whether to check the vertices exist for those using customized id strategy
#vertex.check_customized_id_exist=false

# Whether remove left index at overwrite
#vertex.remove_left_index_at_overwrite=false

# Whether to check the adjacent vertices of edges exist
#vertex.check_adjacent_vertex_exist=false

# Whether to lazy load adjacent vertices of edges
#vertex.lazy_load_adjacent_vertex=true

# Whether to enable the mode to commit part of edges of vertex, enabled if commit size > 0, 0 meas disabled
#vertex.part_edge_commit_size=5000

# Whether to encode number value of primary key in vertex id
#vertex.encode_primary_key_number=true

# The max size(items) of vertices(uncommitted) in transaction
#vertex.tx_capacity=10000

# The max size(items) of edges(uncommitted) in transaction
#edge.tx_capacity=10000

# Whether to ignore invalid data of vertex or edge
#query.ignore_invalid_data=true

# Whether to optimize aggregate query(like count) by index
#query.optimize_aggregate_by_index=false

# The size of each batch when querying by batch
#query.batch_size=1000

# The size of each page when querying by paging
#query.page_size=500

# The maximum number of intermediate results to ]intersect indexes when querying by multiple single index properties
#query.index_intersect_threshold=1000

# Whether to enable ramtable for query of adjacent edges
#query.ramtable_enable=false

# The maximum number of vertices in ramtable, generally the largest vertex id is used as capacity
#query.ramtable_vertices_capacity=10000000

# The maximum number of edges in ramtable, include OUT and IN edges
#query.ramtable_edges_capacity=20000000

# The regex specified the illegal format for schema name
#schema.illegal_name_regex=\s+|~.*

# The max cache size(items) of schema cache
#schema.cache_capacity=10000

# The type of vertex cache, allowed values are [l1, l2]
#vertex.cache_type=l1

# The max cache size(items) of vertex cache
#vertex.cache_capacity=10000000
# The expiration time in seconds of vertex cache
#vertex.cache_expire=600

# The type of edge cache, allowed values are [l1, l2]
#edge.cache_type=l1

# The max cache size(items) of edge cache
#edge.cache_capacity=1000000
# The expiration time in seconds of edge cache
#edge.cache_expire=600

# The worker id of snowflake id generator
#snowflake.worker_id=0

# The datacenter id of snowflake id generator
#snowflake.datecenter_id=0

# Whether to force the snowflake long id to be a string
#snowflake.force_string=false

# Thread number to concurrently execute oltp algorithm
#oltp.concurrent_threads=10

# The min depth to enable concurrent oltp algorithm
#oltp.concurrent_depth=10

# The implementation type of collections used in oltp algorithm, allow: JCF, EC, FU
#oltp.collection_type=EC
